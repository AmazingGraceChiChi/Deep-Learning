{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xvSGDbExff_I"
   },
   "source": [
    "# **Homework 7 - Bert (Question Answering)**\n",
    "\n",
    "If you have any questions, feel free to email us at mlta-2022-spring@googlegroups.com\n",
    "\n",
    "\n",
    "\n",
    "Slide:    [Link](https://docs.google.com/presentation/d/1H5ZONrb2LMOCixLY7D5_5-7LkIaXO6AGEaV2mRdTOMY/edit?usp=sharing)　Kaggle: [Link](https://www.kaggle.com/c/ml2022spring-hw7)　Data: [Link](https://drive.google.com/uc?id=1AVgZvy3VFeg0fX-6WQJMHPVrx3A-M1kb)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WGOr_eS3wJJf"
   },
   "source": [
    "## Task description\n",
    "- Chinese Extractive Question Answering\n",
    "  - Input: Paragraph + Question\n",
    "  - Output: Answer\n",
    "\n",
    "- Objective: Learn how to fine tune a pretrained model on downstream task using transformers\n",
    "\n",
    "- Todo\n",
    "    - Fine tune a pretrained chinese BERT model\n",
    "    - Change hyperparameters (e.g. doc_stride)\n",
    "    - Apply linear learning rate decay\n",
    "    - Try other pretrained models\n",
    "    - Improve preprocessing\n",
    "    - Improve postprocessing\n",
    "- Training tips\n",
    "    - Automatic mixed precision\n",
    "    - Gradient accumulation\n",
    "    - Ensemble\n",
    "\n",
    "- Estimated training time (tesla t4 with automatic mixed precision enabled)\n",
    "    - Simple: 8mins\n",
    "    - Medium: 8mins\n",
    "    - Strong: 25mins\n",
    "    - Boss: 2.5hrs\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TJ1fSAJE2oaC"
   },
   "source": [
    "## Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "YPrc4Eie9Yo5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1AVgZvy3VFeg0fX-6WQJMHPVrx3A-M1kb\n",
      "To: /root/jupyter-workspace/Machine Learning/hw7/hw7_data.zip\n",
      "100%|███████████████████████████████████████| 9.57M/9.57M [00:00<00:00, 165MB/s]\n",
      "Archive:  hw7_data.zip\n",
      "  inflating: hw7_dev.json            \n",
      "  inflating: hw7_test.json           \n",
      "  inflating: hw7_train.json          \n",
      "Wed Apr 20 02:17:53 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.86       Driver Version: 470.86       CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100-PCI...  Off  | 00000000:D8:00.0 Off |                    0 |\n",
      "| N/A   27C    P0    33W / 250W |   4510MiB / 40536MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Download link 1\n",
    "!gdown --id '1AVgZvy3VFeg0fX-6WQJMHPVrx3A-M1kb' --output hw7_data.zip\n",
    "\n",
    "# Download Link 2 (if the above link fails) \n",
    "# !gdown --id '1qwjbRjq481lHsnTrrF4OjKQnxzgoLEFR' --output hw7_data.zip\n",
    "\n",
    "# Download Link 3 (if the above link fails) \n",
    "# !gdown --id '1QXuWjNRZH6DscSd6QcRER0cnxmpZvijn' --output hw7_data.zip\n",
    "\n",
    "!unzip -o hw7_data.zip\n",
    "\n",
    "# For this HW, K80 < P4 < T4 < P100 <= T4(fp16) < V100\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TevOvhC03m0h"
   },
   "source": [
    "## Install transformers\n",
    "\n",
    "Documentation for the toolkit:　https://huggingface.co/transformers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "tbxWFX_jpDom"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.5.0\n",
      "  Downloading transformers-4.5.0-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hCollecting regex!=2019.12.17\n",
      "  Downloading regex-2022.3.15-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (764 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m764.9/764.9 KB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting sacremoses\n",
      "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m895.2/895.2 KB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers==4.5.0) (4.62.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers==4.5.0) (3.6.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from transformers==4.5.0) (21.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.5.0) (1.19.5)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers==4.5.0) (2.27.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->transformers==4.5.0) (3.0.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers==4.5.0) (2.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.5.0) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers==4.5.0) (2019.11.28)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->transformers==4.5.0) (1.25.8)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers==4.5.0) (1.0.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers==4.5.0) (1.15.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers==4.5.0) (8.1.2)\n",
      "Installing collected packages: tokenizers, regex, sacremoses, transformers\n",
      "Successfully installed regex-2022.3.15 sacremoses-0.0.49 tokenizers-0.10.3 transformers-4.5.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.3; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# You are allowed to change version of transformers or use other toolkits\n",
    "!pip install transformers==4.5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8dKM4yCh4LI_"
   },
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "WOTHHtWJoahe"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset \n",
    "from transformers import AdamW, BertForQuestionAnswering, BertTokenizerFast, get_linear_schedule_with_warmup\n",
    "from transformers import AutoModelForQuestionAnswering,AutoTokenizer,pipeline\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Fix random seed for reproducibility\n",
    "def same_seeds(seed):\n",
    "\t  torch.manual_seed(seed)\n",
    "\t  if torch.cuda.is_available():\n",
    "\t\t    torch.cuda.manual_seed(seed)\n",
    "\t\t    torch.cuda.manual_seed_all(seed)\n",
    "\t  np.random.seed(seed)\n",
    "\t  random.seed(seed)\n",
    "\t  torch.backends.cudnn.benchmark = False\n",
    "\t  torch.backends.cudnn.deterministic = True\n",
    "same_seeds(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "7pBtSZP1SKQO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: accelerate==0.2.0 in /usr/local/lib/python3.8/dist-packages (0.2.0)\n",
      "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from accelerate==0.2.0) (1.10.0+cu111)\n",
      "Requirement already satisfied: pyaml>=20.4.0 in /usr/local/lib/python3.8/dist-packages (from accelerate==0.2.0) (21.10.1)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from pyaml>=20.4.0->accelerate==0.2.0) (6.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.4.0->accelerate==0.2.0) (3.7.4.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.3; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Change \"fp16_training\" to True to support automatic mixed precision training (fp16)\t\n",
    "fp16_training = True\n",
    "\n",
    "if fp16_training:\n",
    "    !pip install accelerate==0.2.0\n",
    "    from accelerate import Accelerator\n",
    "    accelerator = Accelerator(fp16=True)\n",
    "    device = accelerator.device\n",
    "\n",
    "# Documentation for the toolkit:  https://huggingface.co/docs/accelerate/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2YgXHuVLp_6j"
   },
   "source": [
    "## Load Model and Tokenizer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "xyBCYGjAp3ym"
   },
   "outputs": [],
   "source": [
    "model_name = \"chinese_pretrain_mrc_roberta_wwm_ext_large\" # \"chinese_pretrain_mrc_macbert_large\"\n",
    "\n",
    "# Use in Transformers\n",
    "tokenizer = AutoTokenizer.from_pretrained(f\"luhua/{model_name}\")\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(f\"luhua/{model_name}\")\n",
    "\n",
    "# You can safely ignore the warning message (it pops up because new prediction heads for QA are initialized randomly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Td-GTmk5OW4"
   },
   "source": [
    "## Read Data\n",
    "\n",
    "- Training set: 31690 QA pairs\n",
    "- Dev set: 4131  QA pairs\n",
    "- Test set: 4957  QA pairs\n",
    "\n",
    "- {train/dev/test}_questions:\t\n",
    "  - List of dicts with the following keys:\n",
    "   - id (int)\n",
    "   - paragraph_id (int)\n",
    "   - question_text (string)\n",
    "   - answer_text (string)\n",
    "   - answer_start (int)\n",
    "   - answer_end (int)\n",
    "- {train/dev/test}_paragraphs: \n",
    "  - List of strings\n",
    "  - paragraph_ids in questions correspond to indexs in paragraphs\n",
    "  - A paragraph may be used by several questions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "NvX7hlepogvu"
   },
   "outputs": [],
   "source": [
    "def read_data(file):\n",
    "    with open(file, 'r', encoding=\"utf-8\") as reader:\n",
    "        data = json.load(reader)\n",
    "    return data[\"questions\"], data[\"paragraphs\"]\n",
    "\n",
    "train_questions, train_paragraphs = read_data(\"hw7_train.json\")\n",
    "dev_questions, dev_paragraphs = read_data(\"hw7_dev.json\")\n",
    "test_questions, test_paragraphs = read_data(\"hw7_test.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fm0rpTHq0e4N"
   },
   "source": [
    "## Tokenize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "rTZ6B70Hoxie"
   },
   "outputs": [],
   "source": [
    "# Tokenize questions and paragraphs separately\n",
    "# 「add_special_tokens」 is set to False since special tokens will be added when tokenized questions and paragraphs are combined in datset __getitem__ \n",
    "\n",
    "train_questions_tokenized = tokenizer([train_question[\"question_text\"] for train_question in train_questions], add_special_tokens=False)\n",
    "dev_questions_tokenized = tokenizer([dev_question[\"question_text\"] for dev_question in dev_questions], add_special_tokens=False)\n",
    "test_questions_tokenized = tokenizer([test_question[\"question_text\"] for test_question in test_questions], add_special_tokens=False) \n",
    "\n",
    "train_paragraphs_tokenized = tokenizer(train_paragraphs, add_special_tokens=False)\n",
    "dev_paragraphs_tokenized = tokenizer(dev_paragraphs, add_special_tokens=False)\n",
    "test_paragraphs_tokenized = tokenizer(test_paragraphs, add_special_tokens=False)\n",
    "\n",
    "# You can safely ignore the warning message as tokenized sequences will be futher processed in datset __getitem__ before passing to model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ws8c8_4d5UCI"
   },
   "source": [
    "## Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "Xjooag-Swnuh"
   },
   "outputs": [],
   "source": [
    "class QA_Dataset(Dataset):\n",
    "    def __init__(self, split, questions, tokenized_questions, tokenized_paragraphs):\n",
    "        self.split = split\n",
    "        self.questions = questions\n",
    "        self.tokenized_questions = tokenized_questions\n",
    "        self.tokenized_paragraphs = tokenized_paragraphs\n",
    "        self.max_question_len = 40\n",
    "        self.max_paragraph_len = 400\n",
    "        \n",
    "        ##### TODO: Change value of doc_stride #####\n",
    "        self.doc_stride = 100\n",
    "\n",
    "        # Input sequence length = [CLS] + question + [SEP] + paragraph + [SEP]\n",
    "        self.max_seq_len = 1 + self.max_question_len + 1 + self.max_paragraph_len + 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        question = self.questions[idx]\n",
    "        tokenized_question = self.tokenized_questions[idx]\n",
    "        tokenized_paragraph = self.tokenized_paragraphs[question[\"paragraph_id\"]]\n",
    "\n",
    "        ##### TODO: Preprocessing #####\n",
    "        # Hint: How to prevent model from learning something it should not learn\n",
    "\n",
    "        if self.split == \"train\":\n",
    "            # Convert answer's start/end positions in paragraph_text to start/end positions in tokenized_paragraph  \n",
    "            answer_start_token = tokenized_paragraph.char_to_token(question[\"answer_start\"])\n",
    "            answer_end_token = tokenized_paragraph.char_to_token(question[\"answer_end\"])\n",
    "\n",
    "            # A single window is obtained by slicing the portion of paragraph containing the answer\n",
    "            mid = (answer_start_token + answer_end_token) // 2\n",
    "            paragraph_start = max(0, min(mid - self.max_paragraph_len // 2, len(tokenized_paragraph) - self.max_paragraph_len))\n",
    "            paragraph_end = paragraph_start + self.max_paragraph_len\n",
    "            \n",
    "            # Slice question/paragraph and add special tokens (101: CLS, 102: SEP)\n",
    "            input_ids_question = [101] + tokenized_question.ids[:self.max_question_len] + [102] \n",
    "            input_ids_paragraph = tokenized_paragraph.ids[paragraph_start : paragraph_end] + [102]\n",
    "            \n",
    "            # Convert answer's start/end positions in tokenized_paragraph to start/end positions in the window  \n",
    "            answer_start_token += len(input_ids_question) - paragraph_start\n",
    "            answer_end_token += len(input_ids_question) - paragraph_start\n",
    "            \n",
    "            # Pad sequence and obtain inputs to model \n",
    "            input_ids, token_type_ids, attention_mask = self.padding(input_ids_question, input_ids_paragraph)\n",
    "            return torch.tensor(input_ids), torch.tensor(token_type_ids), torch.tensor(attention_mask), answer_start_token, answer_end_token\n",
    "\n",
    "        # Validation/Testing\n",
    "        else:\n",
    "            input_ids_list, token_type_ids_list, attention_mask_list = [], [], []\n",
    "            \n",
    "            # Paragraph is split into several windows, each with start positions separated by step \"doc_stride\"\n",
    "            for i in range(0, len(tokenized_paragraph), self.doc_stride):\n",
    "                \n",
    "                # Slice question/paragraph and add special tokens (101: CLS, 102: SEP)\n",
    "                input_ids_question = [101] + tokenized_question.ids[:self.max_question_len] + [102]\n",
    "                input_ids_paragraph = tokenized_paragraph.ids[i : i + self.max_paragraph_len] + [102]\n",
    "                \n",
    "                # Pad sequence and obtain inputs to model\n",
    "                input_ids, token_type_ids, attention_mask = self.padding(input_ids_question, input_ids_paragraph)\n",
    "                \n",
    "                input_ids_list.append(input_ids)\n",
    "                token_type_ids_list.append(token_type_ids)\n",
    "                attention_mask_list.append(attention_mask)\n",
    "            \n",
    "            return torch.tensor(input_ids_list), torch.tensor(token_type_ids_list), torch.tensor(attention_mask_list)\n",
    "\n",
    "    def padding(self, input_ids_question, input_ids_paragraph):\n",
    "        # Pad zeros if sequence length is shorter than max_seq_len\n",
    "        padding_len = self.max_seq_len - len(input_ids_question) - len(input_ids_paragraph)\n",
    "        # Indices of input sequence tokens in the vocabulary\n",
    "        input_ids = input_ids_question + input_ids_paragraph + [0] * padding_len\n",
    "        # Segment token indices to indicate first and second portions of the inputs. Indices are selected in [0, 1]\n",
    "        token_type_ids = [0] * len(input_ids_question) + [1] * len(input_ids_paragraph) + [0] * padding_len\n",
    "        # Mask to avoid performing attention on padding token indices. Mask values selected in [0, 1]\n",
    "        attention_mask = [1] * (len(input_ids_question) + len(input_ids_paragraph)) + [0] * padding_len\n",
    "        \n",
    "        return input_ids, token_type_ids, attention_mask\n",
    "\n",
    "train_set = QA_Dataset(\"train\", train_questions, train_questions_tokenized, train_paragraphs_tokenized)\n",
    "dev_set = QA_Dataset(\"dev\", dev_questions, dev_questions_tokenized, dev_paragraphs_tokenized)\n",
    "test_set = QA_Dataset(\"test\", test_questions, test_questions_tokenized, test_paragraphs_tokenized)\n",
    "\n",
    "train_batch_size = 8\n",
    "\n",
    "# Note: Do NOT change batch size of dev_loader / test_loader !\n",
    "# Although batch size=1, it is actually a batch consisting of several windows from the same QA pair\n",
    "train_loader = DataLoader(train_set, batch_size=train_batch_size, shuffle=True, pin_memory=True)\n",
    "dev_loader = DataLoader(dev_set, batch_size=1, shuffle=False, pin_memory=True)\n",
    "test_loader = DataLoader(test_set, batch_size=1, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5_H1kqhR8CdM"
   },
   "source": [
    "## Function for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "SqeA3PLPxOHu"
   },
   "outputs": [],
   "source": [
    "def evaluate(data, output, question, paragraph):\n",
    "    ##### TODO: Postprocessing #####\n",
    "    # There is a bug and room for improvement in postprocessing \n",
    "    # Hint: Open your prediction file to see what is wrong \n",
    "    \n",
    "    answer = ''\n",
    "    max_prob = float('-inf')\n",
    "    num_of_windows = data[0].shape[1]\n",
    "    \n",
    "    for k in range(num_of_windows):\n",
    "        # Obtain answer by choosing the most probable start position / end position\n",
    "        start_prob, start_index = torch.max(output.start_logits[k], dim=0)\n",
    "        end_prob, end_index = torch.max(output.end_logits[k], dim=0)\n",
    "        \n",
    "        # Probability of answer is calculated as sum of start_prob and end_prob\n",
    "        prob = start_prob + end_prob\n",
    "        \n",
    "        if start_index > end_index or abs(start_index - end_index) >= 20:\n",
    "            prob = 0\n",
    "\n",
    "        # Replace answer if calculated probability is larger than previous windows\n",
    "        if prob > max_prob:\n",
    "            max_prob = prob\n",
    "            # Convert tokens to chars (e.g. [1920, 7032] --> \"大 金\")\n",
    "            answer = tokenizer.decode(data[0][0][k][start_index : end_index + 1])\n",
    "    \n",
    "    # Remove spaces in answer (e.g. \"大 金\" --> \"大金\")\n",
    "    answer = answer.replace(' ','')\n",
    "    \n",
    "    if '[UNK]' in answer:\n",
    "        answer = answer.replace('[UNK]', '*')                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
    "        idx = answer.index('*')\n",
    "        Id = question['id']\n",
    "        para_id = question['paragraph_id']\n",
    "        para_text = paragraph[para_id]\n",
    "        \n",
    "        if idx == 0:\n",
    "            answer_next = answer[1:]\n",
    "            if answer_next in para_text:\n",
    "                next_idx = para_text.index(answer_next)\n",
    "                answer = answer.replace('*', para_text[next_idx-1])\n",
    "        if idx == len(answer) - 1:\n",
    "            answer_prev = answer[:-1]\n",
    "            if answer_prev in para_text:\n",
    "                prev_idx = para_text.index(answer_prev)\n",
    "                answer = answer.replace('*', para_text[prev_idx+len(answer_prev)])\n",
    "        else:\n",
    "            answer_prev = answer[:idx]\n",
    "            answer_next = answer[idx+1:]\n",
    "            if answer_prev in para_text and answer_next in para_text:\n",
    "                if len(answer_next) > len(answer_prev):\n",
    "                    next_idx = para_text.index(answer_next)\n",
    "                    answer = answer.replace('*', para_text[next_idx-1])\n",
    "                else:\n",
    "                    prev_idx = para_text.index(answer_prev)\n",
    "                    answer = answer.replace('*', para_text[prev_idx+len(answer_prev)])\n",
    "            \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rzHQit6eMnKG"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "3Q-B6ka7xoCM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a0cf08fe62143df9eb5654c02ef71ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3962 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f2ef2be151e44ecb3266bfdcff9fdc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3962 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Step 100 | loss = 0.904, acc = 0.662\n",
      "Epoch 1 | Step 200 | loss = 0.838, acc = 0.705\n",
      "Epoch 1 | Step 300 | loss = 0.837, acc = 0.695\n",
      "Epoch 1 | Step 400 | loss = 0.782, acc = 0.712\n",
      "Epoch 1 | Step 500 | loss = 0.738, acc = 0.697\n",
      "Epoch 1 | Step 600 | loss = 0.782, acc = 0.715\n",
      "Epoch 1 | Step 700 | loss = 0.866, acc = 0.675\n",
      "Epoch 1 | Step 800 | loss = 0.758, acc = 0.721\n",
      "Epoch 1 | Step 900 | loss = 0.816, acc = 0.680\n",
      "Epoch 1 | Step 1000 | loss = 0.803, acc = 0.684\n",
      "Epoch 1 | Step 1100 | loss = 0.858, acc = 0.691\n",
      "Epoch 1 | Step 1200 | loss = 0.661, acc = 0.740\n",
      "Epoch 1 | Step 1300 | loss = 0.739, acc = 0.704\n",
      "Epoch 1 | Step 1400 | loss = 0.718, acc = 0.740\n",
      "Epoch 1 | Step 1500 | loss = 0.627, acc = 0.762\n",
      "Epoch 1 | Step 1600 | loss = 0.581, acc = 0.779\n",
      "Epoch 1 | Step 1700 | loss = 0.617, acc = 0.772\n",
      "Epoch 1 | Step 1800 | loss = 0.643, acc = 0.769\n",
      "Epoch 1 | Step 1900 | loss = 0.574, acc = 0.757\n",
      "Epoch 1 | Step 2000 | loss = 0.563, acc = 0.774\n",
      "Epoch 1 | Step 2100 | loss = 0.569, acc = 0.761\n",
      "Epoch 1 | Step 2200 | loss = 0.625, acc = 0.759\n",
      "Epoch 1 | Step 2300 | loss = 0.640, acc = 0.775\n",
      "Epoch 1 | Step 2400 | loss = 0.547, acc = 0.770\n",
      "Epoch 1 | Step 2500 | loss = 0.582, acc = 0.764\n",
      "Epoch 1 | Step 2600 | loss = 0.539, acc = 0.782\n",
      "Epoch 1 | Step 2700 | loss = 0.539, acc = 0.797\n",
      "Epoch 1 | Step 2800 | loss = 0.483, acc = 0.816\n",
      "Epoch 1 | Step 2900 | loss = 0.520, acc = 0.794\n",
      "Epoch 1 | Step 3000 | loss = 0.456, acc = 0.825\n",
      "Epoch 1 | Step 3100 | loss = 0.528, acc = 0.809\n",
      "Epoch 1 | Step 3200 | loss = 0.507, acc = 0.790\n",
      "Epoch 1 | Step 3300 | loss = 0.438, acc = 0.840\n",
      "Epoch 1 | Step 3400 | loss = 0.439, acc = 0.827\n",
      "Epoch 1 | Step 3500 | loss = 0.439, acc = 0.812\n",
      "Epoch 1 | Step 3600 | loss = 0.447, acc = 0.812\n",
      "Epoch 1 | Step 3700 | loss = 0.458, acc = 0.817\n",
      "Epoch 1 | Step 3800 | loss = 0.499, acc = 0.808\n",
      "Epoch 1 | Step 3900 | loss = 0.455, acc = 0.820\n",
      "Evaluating Dev Set ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb70fcf467ba4b019ed8837346460b58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4131 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "227 452 李杲\n",
      "389 162 朱允炆\n",
      "392 655 筥崎八幡宮\n",
      "503 565 與慕容廆雙方不和\n",
      "699 846 對日本報紙的無恥造謠誣衊，進行了有力駁斥\n",
      "841 366 杜恆-蒯因論題\n",
      "983 880 青翁三足缶\n",
      "1383 37 **\n",
      "1400 169 朱允炆\n",
      "1442 705 劉炟\n",
      "1511 805 免再次爆發內訌\n",
      "1844 795 艚船\n",
      "1915 48 凱爾特—利古里亞部落\n",
      "2204 265 朱載堉\n",
      "2261 830 學習訓詁學\n",
      "2398 163 朱允炆的禁殺之旨\n",
      "2661 841 李端棻\n",
      "2751 374 w.v.*因\n",
      "3167 1012 **\n",
      "3716 1277 拓跋氏\n",
      "3981 1412 學習訓詁學\n",
      "Validation | Epoch 1 | acc = 0.805\n",
      "Saving Model ...\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 1\n",
    "validation = True\n",
    "logging_step = 100\n",
    "learning_rate = 5e-5\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "if fp16_training:\n",
    "    model, optimizer, train_loader = accelerator.prepare(model, optimizer, train_loader) \n",
    "\n",
    "model.train()\n",
    "\n",
    "print(\"Start Training ...\")\n",
    "\n",
    "total_steps = num_epoch * len(tqdm(train_loader))\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=100, num_training_steps=total_steps)\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    step = 1\n",
    "    train_loss = train_acc = 0\n",
    "    \n",
    "    for data in tqdm(train_loader):\n",
    "        # Load all data into GPU\n",
    "        data = [i.to(device) for i in data]\n",
    "        \n",
    "        # Model inputs: input_ids, token_type_ids, attention_mask, start_positions, end_positions (Note: only \"input_ids\" is mandatory)\n",
    "        # Model outputs: start_logits, end_logits, loss (return when start_positions/end_positions are provided)  \n",
    "        output = model(input_ids=data[0], token_type_ids=data[1], attention_mask=data[2], start_positions=data[3], end_positions=data[4])\n",
    "\n",
    "        # Choose the most probable start position / end position\n",
    "        start_index = torch.argmax(output.start_logits, dim=1)\n",
    "        end_index = torch.argmax(output.end_logits, dim=1)\n",
    "        \n",
    "        # Prediction is correct only if both start_index and end_index are correct\n",
    "        train_acc += ((start_index == data[3]) & (end_index == data[4])).float().mean()\n",
    "        train_loss += output.loss\n",
    "        \n",
    "        if fp16_training:\n",
    "            accelerator.backward(output.loss)\n",
    "        else:\n",
    "            output.loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()  \n",
    "        optimizer.zero_grad()\n",
    "        step += 1\n",
    "        \n",
    "        # Print training loss and accuracy over past logging step\n",
    "        if step % logging_step == 0:\n",
    "            print(f\"Epoch {epoch + 1} | Step {step} | loss = {train_loss.item() / logging_step:.3f}, acc = {train_acc / logging_step:.3f}\")\n",
    "            train_loss = train_acc = 0\n",
    "\n",
    "    if validation:\n",
    "        print(\"Evaluating Dev Set ...\")\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            dev_acc = 0\n",
    "            for i, data in enumerate(tqdm(dev_loader)):\n",
    "                question = dev_questions[i]\n",
    "                output = model(input_ids=data[0].squeeze(dim=0).to(device), token_type_ids=data[1].squeeze(dim=0).to(device),\n",
    "                       attention_mask=data[2].squeeze(dim=0).to(device))\n",
    "                # prediction is correct only if answer text exactly matches\n",
    "                dev_acc += evaluate(data, output, question, dev_paragraphs) == dev_questions[i][\"answer_text\"]\n",
    "            print(f\"Validation | Epoch {epoch + 1} | acc = {dev_acc / len(dev_loader):.3f}\")\n",
    "        model.train()\n",
    "\n",
    "# Save a model and its configuration file to the directory 「saved_model」 \n",
    "# i.e. there are two files under the direcory 「saved_model」: 「pytorch_model.bin」 and 「config.json」\n",
    "# Saved model can be re-loaded using 「model = BertForQuestionAnswering.from_pretrained(\"saved_model\")」\n",
    "print(\"Saving Model ...\")\n",
    "model_save_dir = \"saved_model\" \n",
    "model.save_pretrained(model_save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kMmdLOKBMsdE"
   },
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "U5scNKC9xz0C"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Test Set ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9e58b6cf0cd46ae8eff11c578233d84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4957 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250 626 溥儁\n",
      "332 47 目前沒有觀察到任何語言純綷以力道來區分不同輔音\n",
      "340 176 荇人國\n",
      "428 85 ㎝左右\n",
      "563 566 馬馼\n",
      "635 265 東晉常璩\n",
      "938 90 對稻\n",
      "991 113 白堊紀滅絕事件\n",
      "1423 46 抗佝僂病維他命\n",
      "1534 554 杭州筧橋機場\n",
      "1557 695 蔡鍔\n",
      "1680 671 丁旿\n",
      "1750 291 隋朝帝\n",
      "1913 378 胡季犛\n",
      "2042 154 其英文縮寫首字母為「ㄟ·ㄎㄟ·ㄨㄞ\n",
      "2398 257 梁鵠\n",
      "2512 991 韃靼海峽\n",
      "2601 45 部分商人會利用品種差異以虹鱒混充鮭魚\n",
      "2619 113 白堊紀末滅絕事件\n",
      "2767 956 侏儸紀\n",
      "3026 981 克里米亞韃靼人\n",
      "3137 93 於糯\n",
      "3369 623 白堊紀中期\n",
      "3606 1054 白堊紀\n",
      "Completed! Result is in result.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating Test Set ...\")\n",
    "\n",
    "result = []\n",
    "    \n",
    "model.eval()\n",
    "\n",
    "i = 0\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(test_loader):\n",
    "        question = test_questions[i]\n",
    "        output = model(input_ids=data[0].squeeze(dim=0).to(device), token_type_ids=data[1].squeeze(dim=0).to(device),\n",
    "                       attention_mask=data[2].squeeze(dim=0).to(device))\n",
    "        result.append(evaluate(data, output, question, test_paragraphs))\n",
    "        i += 1\n",
    "result_file = \"result.csv\"\n",
    "with open(result_file, 'w') as f:\n",
    "\t  f.write(\"ID,Answer\\n\")\n",
    "\t  for i, test_question in enumerate(test_questions):\n",
    "        # Replace commas in answers with empty strings (since csv is separated by comma)\n",
    "        # Answers in kaggle are processed in the same way\n",
    "\t\t    f.write(f\"{test_question['id']},{result[i].replace(',','')}\\n\")\n",
    "\n",
    "print(f\"Completed! Result is in {result_file}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ML2022Spring - HW7.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
